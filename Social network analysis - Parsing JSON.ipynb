{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import text from the JSON file for SNA\n",
    "\n",
    "Go through all comments (and comments of comments) and convert each post into a row of the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1, Create the column names for the output dataframe to store post information extracted from the .json files.\n",
    "\n",
    "This cell reads the element names in CSMM101_3T2020_discussion_threads.json as the column names.\n",
    "\n",
    "We also add and remove some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['url', 'type', 'title', 'category', 'subcategory', 'votes', 'private', 'created_at', 'text', 'user_from', 'user_from_role', 'user_from_email', 'user_to', 'user_to_role', 'user_to_email']\n"
     ]
    }
   ],
   "source": [
    "with open(\"./CSMM101_3T2020_discussion_threads.json\") as ft:\n",
    "        discussions_t = json.load(ft)\n",
    "\n",
    "ft.close()\n",
    "\n",
    "# Create a dataframe (df_SNA) to store data for every post, comment, and answer.\n",
    "df_col_names = [str(element) for element in list(discussions_t[0].keys())]\n",
    "df_col_names += ['user_from']\n",
    "df_col_names += ['user_from_role']\n",
    "df_col_names += ['user_from_email']\n",
    "df_col_names += ['user_to']\n",
    "df_col_names += ['user_to_role']\n",
    "df_col_names += ['user_to_email']\n",
    "df_col_names.remove('user')\n",
    "df_col_names.remove('document')\n",
    "df_col_names.remove('comments')\n",
    "\n",
    "print(df_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2, Check the .csv files that store user information: numbers of emails, user ids, and user names should be the same.\n",
    "\n",
    "Also, please note that the Username in gradebook and user name in json file do not match. So we use email address to\n",
    "link the two dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Beta1_Data/ColumbiaX_CSMM103x_3T2019_grade_report_2019-12-29-2126.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n",
      "./Beta1_Data/ColumbiaX_BAMM103x_3T2019_grade_report_2019-12-29-2121.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n",
      "./Beta1_Data/ColumbiaX_CSMM101x_3T2019_grade_report_2019-12-29-0248.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n",
      "./Beta1_Data/ColumbiaX_BAMM104x_3T2019_grade_report_2019-12-29-2127.csv\n",
      "5967 5967 5966\n",
      "./Beta1_Data/ColumbiaX_CSMM104x_3T2019_grade_report_2019-12-29-2119.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n",
      "./Beta1_Data/ColumbiaX_BAMM102x_3T2019_grade_report_2019-12-29-2122.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n",
      "./Beta1_Data/ColumbiaX_CSMM102x_3T2019_grade_report_2019-12-29-2126.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n",
      "./Beta1_Data/ColumbiaX_BAMM101x_3T2019_grade_report_2019-12-29-2124.csv\n",
      "Numbers of Emails, Student IDs, and Names all match.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_users(user_file):\n",
    "    user_csv = pd.read_csv(user_file, sep = ',')\n",
    "    uni_emails = len(user_csv['Email'].unique())\n",
    "    uni_stuid = len(user_csv['Student ID'].unique())\n",
    "    uni_names = len(user_csv['Username'].unique())\n",
    "\n",
    "    if uni_emails == uni_stuid & uni_stuid == uni_names :\n",
    "        print(\"Numbers of Emails, Student IDs, and Names all match.\\n\")\n",
    "    else :\n",
    "        print(uni_emails, uni_stuid, uni_names)\n",
    "\n",
    "data_dir = './Beta1_Data/'\n",
    "for fname in os.listdir(data_dir) :\n",
    "    if fname.endswith(\".csv\"):\n",
    "        print(os.path.join(data_dir, fname))\n",
    "        check_users(os.path.join(data_dir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3, Define json_to_csv function. Input is the json file and output is the dataframe of the flattened records."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def json_to_csv(json_f):\n",
    "\n",
    "    # read json file\n",
    "    with open(str(data_dir + json_f)) as f:\n",
    "        discussions = json.load(f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    result = pd.DataFrame([], columns = df_col_names)\n",
    "\n",
    "    for discussion in discussions:\n",
    "        user = {'name': None, 'role': None, 'email': None}\n",
    "        result = read_comment(discussion, result, user, None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4, Define read_comment function, which converts each post/comment to a row in the output dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read_comment is a recursive function to read all comments (and answers)\n",
    "def read_comment(comment, output, user_to_passed, post_type_passed):\n",
    "    # List of input vars\n",
    "    #   comment: dict type. the post (original post or its comment, including comment of comment) to be read.\n",
    "    #   output: dataframe type. output is a dataframe that treat each post (and comment) as a row.\n",
    "    #   user_to_passed: dict type, {'name', 'role'}.\n",
    "    #       to pass user information from upper level post (comment) to next level.\n",
    "    #   post_type_passed: str type. similar to user_to_passed, but to pass post type.\n",
    "\n",
    "    # read post author of the current level post\n",
    "    # author = comment['user']['name']\n",
    "    # author_role = comment['user']['role']\n",
    "    # author_email = comment['user']['email']\n",
    "    author = comment['user']\n",
    "\n",
    "    # create user info dict to pass to next level\n",
    "    user_to = {'name': None, 'role': None, 'email': None}\n",
    "\n",
    "    # Inherit post information from the record (row) above.\n",
    "    # Because comments usually do not have 'type', 'title', 'category', 'subcategory', or 'private'\n",
    "    for elem in df_col_names[0:-8] :\n",
    "        if elem not in comment :\n",
    "            #print(\"elem not in list\")\n",
    "            #print(len(output))\n",
    "            comment[elem] = output.iloc[-1, output.columns.get_loc(elem)]\n",
    "\n",
    "    # If comments exist, author info will be passed to next level posts as user_to.\n",
    "    if comment['comments'] : # comments is not empty\n",
    "        user_to = author\n",
    "        # user_to['name'] = author\n",
    "        # user_to['role'] = author_role\n",
    "        # user_to['email'] = author_email\n",
    "    # else : # comments is empty\n",
    "        # user_to['name'] = None\n",
    "        # user_to['role'] = None\n",
    "        # user_to['email'] = None\n",
    "\n",
    "    # Answers will be marked as answer type.\n",
    "    if post_type_passed == 'answer' :\n",
    "        comment['type'] = post_type_passed\n",
    "\n",
    "    # Add new record to existing output dataframe.\n",
    "    # Each new record (row) contains all information related to the post.\n",
    "    output = output.append({'url': comment['url'],\n",
    "                    'type': comment['type'],\n",
    "                    'title': comment['title'],\n",
    "                    'category': comment['category'],\n",
    "                    'subcategory': comment['subcategory'],\n",
    "                    'votes': comment['votes'],\n",
    "                    'private': comment['private'],\n",
    "                    'created_at': comment['created_at'],\n",
    "                    'text': comment['text'],\n",
    "                    'user_from': author['name'],\n",
    "                    'user_from_role': author['role'],\n",
    "                    'user_from_email': author['email'],\n",
    "                    'user_to': user_to_passed['name'],\n",
    "                    'user_to_role': user_to_passed['role'],\n",
    "                    'user_to_email': user_to_passed['email']}, ignore_index=True)\n",
    "\n",
    "    # Go over all comments/answers\n",
    "    replies = comment['comments']\n",
    "    for reply in replies:\n",
    "        # recursive part\n",
    "        output = read_comment(reply, output, user_to, None)\n",
    "\n",
    "        # replier = reply['user']['name']\n",
    "        # replier_role = reply['user']['role']\n",
    "        # replier_email = reply['user']['email']\n",
    "        #\n",
    "        # output.iloc[-1, output.columns.get_loc('user_from')] = replier\n",
    "        # output.iloc[-1, output.columns.get_loc('user_from_role')] = replier_role\n",
    "        # output.iloc[-1, output.columns.get_loc('user_from_email')] = replier_email\n",
    "\n",
    "    if 'answers' in comment :\n",
    "        answers = comment['answers']\n",
    "        user_to = author\n",
    "        # user_to['name'] = author\n",
    "        # user_to['role'] = author_role\n",
    "        # user_to['email'] = author_email\n",
    "        for answer in answers:\n",
    "            output = read_comment(answer, output, user_to, 'answer')\n",
    "\n",
    "            # replier = answer['user']['name']\n",
    "            # replier_role = answer['user']['role']\n",
    "            # replier_email = answer['user']['email']\n",
    "            # output.iloc[-1, output.columns.get_loc('user_from')] = replier\n",
    "            # output.iloc[-1, output.columns.get_loc('user_from_role')] = replier_role\n",
    "            # output.iloc[-1, output.columns.get_loc('user_from_email')] = replier_email\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5, Prepare dfbeta1_master.csv for SNA\n",
    "For all 8 courses, read .json files and create a dataframe to store all posts/comments. Then read .csv files for the\n",
    "user information, and add the enrollment track to the dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing course BAMM101 \n",
      ".json file is BAMM101_ Analytics in Python 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_BAMM101x_3T2019_grade_report_2019-12-29-2124.csv\n",
      "Processing course BAMM102 \n",
      ".json file is BAMM102_ Data, Model and Decisions 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_BAMM102x_3T2019_grade_report_2019-12-29-2122.csv\n",
      "Processing course BAMM103 \n",
      ".json file is BAMM103_ Demand and Supply Analytics 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_BAMM103x_3T2019_grade_report_2019-12-29-2121.csv\n",
      "Processing course BAMM104 \n",
      ".json file is BAMM104_ Marketing Analytics 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_BAMM104x_3T2019_grade_report_2019-12-29-2127.csv\n",
      "Processing course CSMM101 \n",
      ".json file is CSMM101_ Artificial Intelligence 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_CSMM101x_3T2019_grade_report_2019-12-29-0248.csv\n",
      "Processing course CSMM102 \n",
      ".json file is CSMM102_ Machine Learning 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_CSMM102x_3T2019_grade_report_2019-12-29-2126.csv\n",
      "Processing course CSMM103 \n",
      ".json file is CSMM103_ Robotics 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_CSMM103x_3T2019_grade_report_2019-12-29-2126.csv\n",
      "Processing course CSMM104 \n",
      ".json file is CSMM104_ Animation and CGI Motion 3T2019 discussion threads.json \n",
      ".csv file is  ColumbiaX_CSMM104x_3T2019_grade_report_2019-12-29-2119.csv\n"
     ]
    }
   ],
   "source": [
    "# define path of folder where all data files are stored.\n",
    "data_dir = './Beta1_Data/'\n",
    "# define tags of the courses.\n",
    "course_list = ['BAMM101', 'BAMM102', 'BAMM103', 'BAMM104', 'CSMM101', 'CSMM102', 'CSMM103', 'CSMM104']\n",
    "\n",
    "# define the final output dataframe colnames.\n",
    "df_col_names = ['url', 'type', 'title', 'category', 'subcategory', 'votes', 'private', 'created_at',\n",
    "                'text', 'user_from', 'user_from_role', 'user_from_email', 'user_to', 'user_to_role', 'user_to_email']\n",
    "\n",
    "json_file = '' # json file name\n",
    "user_file = '' # user file name\n",
    "append_switch = False # indicator of whether to create a new csv file or append data to existing file.\n",
    "\n",
    "# for each course (course tag), find the json_file and user_file to prepare final output dataframe.\n",
    "for keyword in course_list :\n",
    "    for fname in os.listdir(data_dir) :\n",
    "        if keyword in fname :\n",
    "            if '.json' in fname : json_file = fname\n",
    "            if '.csv' in fname : user_file = fname\n",
    "\n",
    "    print(\"Processing course\", keyword, \"\\n.json file is\", json_file, \"\\n.csv file is \", user_file)\n",
    "\n",
    "    # create output_df as a dataframe to store all records (posts and comments) in the json file.\n",
    "    output_df = json_to_csv(json_file)\n",
    "\n",
    "    # add course_id and thread_id to output_df\n",
    "    output_df['course_id'] = keyword\n",
    "    # create thread_id based on url.\n",
    "    output_df['thread_id'] = output_df['url']\n",
    "    output_df['thread_id'] = output_df['url'].str.split(\"?\").str.get(0).str.split(\"courses/\").str.get(1)\n",
    "\n",
    "    # link user information from user_file: we need the enrollment track for each user_from and user_to\n",
    "    output_df['user_from_track'] = \"\"\n",
    "    output_df['user_to_track'] = \"\"\n",
    "\n",
    "    # read user information\n",
    "    user_raw = pd.read_csv(str(data_dir + user_file), sep = ',')\n",
    "    user_df = user_raw[[\"Username\", \"Email\", \"Enrollment Track\"]]\n",
    "\n",
    "    # link two datasets. two approaches could be use: 1, pd.merge; 2, for each post/comment, find user_from/user_to in\n",
    "    # the user_df, and pass the Enrollment Track value to output_df.\n",
    "    # Please note that the user names in two datasets do not always match. So, user's email address is used as the keys.\n",
    "\n",
    "    # Approach 1: pd.merge. This is quicker.\n",
    "    track_df = pd.merge(output_df['user_from_email'], user_df, left_on=\"user_from_email\", right_on=\"Email\", how=\"left\")\n",
    "    output_df['user_from_track'] = track_df['Enrollment Track']\n",
    "\n",
    "    track_df = pd.merge(output_df['user_to_email'], user_df, left_on=\"user_to_email\", right_on=\"Email\", how=\"left\")\n",
    "    output_df['user_to_track'] = track_df['Enrollment Track']\n",
    "\n",
    "    # # Approach 2: for each post/comment, find user_from/user_to in the user_df, and pass the Enrollment Track value to output_df.\n",
    "    # for i in range(0, len(output_df)) :\n",
    "    #     current_record = output_df.iloc[i].copy()\n",
    "    #\n",
    "    #     enroll_track = user_df.loc[user_df['Email'] == current_record['user_from_email'], 'Enrollment Track']\n",
    "    #     if len(enroll_track) != 0 :\n",
    "    #         output_df.iloc[i, output_df.columns.get_loc('user_from_track')] = enroll_track.iloc[0]\n",
    "    #\n",
    "    #     enroll_track = user_df.loc[user_df['Email'] == current_record['user_to_email'], 'Enrollment Track']\n",
    "    #     if len(enroll_track) != 0 :\n",
    "    #         output_df.iloc[i, output_df.columns.get_loc('user_to_track')] = enroll_track.iloc[0]\n",
    "\n",
    "    # Save to csv file.\n",
    "    if not append_switch : # first course, so new file will be created.\n",
    "        output_df.to_csv(\"./dfbeta1_master.csv\", sep=',', index=False, mode='w')\n",
    "\n",
    "    if append_switch : # following courses, so output_df will be appended.\n",
    "        output_df.to_csv(\"./dfbeta1_master.csv\", sep=',', index=False, mode='a', header=False)\n",
    "        \n",
    "    append_switch = True\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}